{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Módulo # 4 - Big Data\n",
    "\n",
    "## Tarea # 3 \n",
    "## Autor: Jose Martinez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datos de Entrada\n",
    "\n",
    "## Abandono de Banco\n",
    "\n",
    "Este conjunto de datos contiene detalles de los clientes de un banco y la variable objetivo es una variable binaria que refleja el hecho de si el cliente dejó el banco (cerró su cuenta) o si continúa siendo un cliente.\n",
    "\n",
    "### Features\n",
    "\n",
    "- `RowNumber`: Número de fila (Int)\n",
    "- `CustomerId`: Identificador del cliente (Int)\n",
    "- `Surname`: apellido del cliente (String)\n",
    "- `CreditScore`: puntaje de crédito del cliente (Number)\n",
    "- `Geography`: geografía del cliente (String)\n",
    "- `Gender`: Sexo del cliente (String)\n",
    "- `Age`: Edad del cliente (Int)\n",
    "- `Tenure`: Número de años que el cliente ha estado en el banco (Int)\n",
    "- `Balance`: estado de cuenta del cliente (Float)\n",
    "- `NumOfProducts`: numero de productos del cliente (Int)\n",
    "- `HasCrCard`: tiene tarjeta de crédito (Bool)\n",
    "- `IsActiveMember`: es un miembro activo (Bool)\n",
    "- `EstimatedSalary`: salario estimado del cliente (Float)\n",
    "\n",
    "### Objetivo Predictivo\n",
    "\n",
    "- `Exited`: si el cliente abandonó el banco (Bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/usr/lib/python3.7/site-packages/pyspark')\n",
    "\n",
    "from pyspark.sql.types import (StringType, IntegerType, FloatType, \n",
    "                                StructField, StructType)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Bigdata: Tarea 3\") \\\n",
    "    .config(\"spark.driver.extraClassPath\", \"postgresql-42.2.14.jar\") \\\n",
    "    .config(\"spark.executor.extraClassPath\", \"postgresql-42.2.14.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define the schema of the dataframe\n",
    "churn_df = spark \\\n",
    "    .read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"path\", \"churn_modelling.csv\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .schema(StructType([\n",
    "                StructField(\"RowNumber\", IntegerType()),\n",
    "                StructField(\"CustomerId\", IntegerType()),\n",
    "                StructField(\"Surname\", StringType()),\n",
    "                StructField(\"CreditScore\", IntegerType()),\n",
    "                StructField(\"Geography\", StringType()),\n",
    "                StructField(\"Gender\", StringType()),\n",
    "                StructField(\"Age\", IntegerType()),\n",
    "                StructField(\"Tenure\", IntegerType()),\n",
    "                StructField(\"Balance\", FloatType()),\n",
    "                StructField(\"NumOfProducts\", IntegerType()),\n",
    "                StructField(\"HasCrCard\", IntegerType()),\n",
    "                StructField(\"IsActiveMember\", IntegerType()),\n",
    "                StructField(\"EstimatedSalary\", FloatType()),\n",
    "                StructField(\"Exited\", IntegerType())])) \\\n",
    "    .load()\n",
    "\n",
    "# Print the schema of the dataframe\n",
    "churn_df.printSchema()\n",
    "churn_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "# Se hace un primer filtrado para eliminar los registros que no tienen\n",
    "# información valiosa para calcular el modelo. Como lo son los registros\n",
    "# RowNumber, CustomerId, Surname, Geography. \n",
    "\n",
    "columns_kept = ['CreditScore', 'Gender', 'Age', 'Tenure', \n",
    "                'Balance', 'NumOfProducts', 'HasCrCard',\n",
    "                'IsActiveMember', 'EstimatedSalary', 'Exited']\n",
    "                \n",
    "selected_columns_df = churn_df.select(columns_kept)\n",
    "\n",
    "# Change Gender to binary int\n",
    "selected_columns_df = selected_columns_df.withColumn('Gender',\n",
    "                                                     when(selected_columns_df.Gender == 'Male', 1)\n",
    "                                                     .when(selected_columns_df.Gender == 'Female', 0)\n",
    "                                                     .otherwise(selected_columns_df.Gender))\n",
    "selected_columns_df = selected_columns_df .withColumn('Gender', selected_columns_df['Gender'].cast(IntegerType()))\n",
    "\n",
    "selected_columns_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gráficos y Estadísticas Descriptivas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimimos información de los datos para verificar que no hay ningún\n",
    "# problema con los datos.\n",
    "selected_columns_df.describe(['CreditScore', 'Gender', 'Age', 'Tenure']).show()\n",
    "selected_columns_df.describe(['Balance', 'NumOfProducts', 'HasCrCard']).show()\n",
    "selected_columns_df.describe(['IsActiveMember', 'EstimatedSalary', 'Exited']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Para realizar operaciones más detalladas es necesario expresar las filas originales en vectores\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=['CreditScore', 'Gender', 'Age', 'Tenure', \n",
    "                'Balance', 'NumOfProducts', 'HasCrCard',\n",
    "                'IsActiveMember', 'EstimatedSalary'],\n",
    "    outputCol='features')\n",
    "\n",
    "vector_df = assembler.transform(selected_columns_df)\n",
    "vector_df = vector_df.select(['features', 'Exited'])\n",
    "vector_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Con la representación de vectores podemos calcular correlaciones\n",
    "from pyspark.ml.stat import Correlation\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pearson_matrix = Correlation.corr(vector_df, 'features').collect()[0][0]\n",
    "\n",
    "sns.heatmap(pearson_matrix.toArray(), annot=True, fmt=\".2f\", cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputacion de valores faltantes\n",
    "\n",
    "El dataset fue revisando previamente para ver si existen valores faltantes. En este se cuanta con la fortuna de que no existen valores faltantes, por lo que no es necesario realizar ninguna acción. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalización / Estandarización de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler, Normalizer\n",
    "\n",
    "standard_normalizer = Normalizer(inputCol='features', outputCol='normFeatures')\n",
    "normalize_df = standard_normalizer.transform(vector_df)\n",
    "normalize_df.show()\n",
    "\n",
    "standard_scaler = StandardScaler(inputCol='normFeatures', outputCol='scaledFeatures')\n",
    "scale_model = standard_scaler.fit(normalize_df)\n",
    "\n",
    "scaled_df = scale_model.transform(normalize_df)\n",
    "scaled_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Escritura a base de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Convert vector to column\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "\n",
    "pre_df =scaled_df.withColumn('features', vector_to_array('scaledFeatures'))\n",
    "\n",
    "pre_df.show()\n",
    "# Almacenar el conjunto de datos limpio en la base de datos\n",
    "scaled_df \\\n",
    "    .write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .mode('overwrite') \\\n",
    "    .option(\"url\", \"jdbc:postgresql://172.17.0.1:5433/postgres\") \\\n",
    "    .option(\"user\", \"postgres\") \\\n",
    "    .option(\"password\", \"testPassword\") \\\n",
    "    .option(\"dbtable\", \"tarea3\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading single DataFrame in Spark by retrieving all rows from a DB table.\n",
    "df = spark \\\n",
    "    .read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://172.17.0.1:5433/postgres\") \\\n",
    "    .option(\"user\", \"postgres\") \\\n",
    "    .option(\"password\", \"testPassword\") \\\n",
    "    .option(\"dbtable\", \"tarea3\") \\\n",
    "    .load()\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uso de protocolo K-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Split the data into training and test sets (70 % training, 30 % test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo 1: Regresión Logística "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nótese que no se hace partición de datos de entrenamiento (ejercicio posterior).\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=['Features[0]', 'Features[1]', 'Features[2]', 'Features[3]', \n",
    "               'Features[4]', 'Features[5]', 'Features[6]',\n",
    "               'Features[7]', 'Features[8]'],\n",
    "    outputCol='Features')\n",
    "\n",
    "vector_df = assembler.transform(selected_columns_df)\n",
    "vector_df = vector_df.select(['Features', 'Exited'])\n",
    "vector_df.show()\n",
    "\n",
    "regression = LinearRegression(featuresCol='Features', labelCol='Exited')\n",
    "regression_model = regression.fit(df)\n",
    "\n",
    "print('Pesos: {}\\n b: {}'.format(regression_model.coefficients, regression_model.intercept))\n",
    "\n",
    "print('RMSE: {} r2: {}'.format(\n",
    "    regression_model.summary.rootMeanSquaredError,\n",
    "    regression_model.summary.r2))\n",
    "\n",
    "scaled_df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo 2: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluación del conjunto de validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación y almacenado de modelo 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar en base de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación y almacenado de modelo 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar en base de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analisis de resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PORQUE DIO BIEN? "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
